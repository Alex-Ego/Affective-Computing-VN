\chapter{Project Design}
\label{ch4}
The tools that are used in this paper are mostly Python-based, such as TensorFlow, a neural network framework. This, combined with natural language processing tools and several filtering techniques will be used to achieve -- or at least approach as close as possible to -- the expected results.
Having all the concepts in mind, the proposed project has a major component which is the Machine Learning tools surrounded by several optional small modules such as the GUI and chatbot components, which are not the focus of this project.
As for the data used in this project, most of it comes from cleaned, classified tweets partitioned in training and testing datasets.
In this chapter, the design of the project is explained.

\section{Inner Workings Design}
In this section, the data and the relations with the algorithm is explained, with a focus on the design itself.
\subsection{Datasets}
The datasets used in this project, as previously mentioned, consist in around 40,000 semi-clean, classified tweets in 13 categories \citep{rf20}, but as the scope for all of those labels exceeds the one proposed in this paper, the ones taken into consideration are as follows:
\begin{itemize}
	\item Sadness
	\item Neutral
	\item Happiness
	\item Fun
	\item Worry
	\item Boredom
	\item Joy
	\item Love
	\item Fear
\end{itemize}
Even after eliminating non-critical labels, since the remaining  labeled samples are not evenly distributed, leaving them as-is led to very inaccurate results, so a generalistic approach was opted for, classifying the end results in ``Good'', ``Neutral'' and ``Bad'' depending on the overall wellness percieved from the input.
This final filter works only with the training data, and works as follows:
\begin{itemize}
	\item Sadness, Worry and Fear are in the ``Bad'' category.
	\item Neutral and Boredom are in the ``Neutral'' category.
	\item Happiness, Fun, Joy and Love are in the ``Good'' category.
\end{itemize}

\subsection{Text Filtering}
\label{txtfilter}
Since the chosen dataset is imported almost straight from Twitter with poor grammar, misplaced symbols, emojis and similar things, some cleanup has to be done to ensure peak performance.
\begin{itemize}
\item First, all text must be converted to lowercase.
\item Then, all of the punctuation marks had to be discarded.
\item After that, the stopwords\footnote{Words that are not vital for the sentence's meaning.} have to be omitted as well.
\item Finally, for easier analysis, a process called stemming\footnote{Trimming words down by removing the affixes like -ed, -ize, -s, and so on.} is applied, so that all tenses of every verb and all related words are evaluated the same way while also avoiding corpus bloating.
\end{itemize} 
These last processes were possible thanks to NLTK\footnote{Natural Language Toolkit, tool used specifically for these case scenarios. \url{https://www.nltk.org/}}, which has its own repository of stopwords and stems.
An example for this applied to data in the training dataset is as follows.
\begin{center}
\fbox{So sleepy again and it's not even that late. I fail once again.}
\end{center}
Following the filtering order, first all the characters are converted to lowercase.
\begin{center}
\fbox{so sleepy again and it's not even that late. i fail once again.}
\end{center}
After that, the text is stripped from all non-alphabetic characters.
\begin{center}
\fbox{so sleepy again and it s not even that late i fail once again}
\end{center}
Next, all stopwords are culled from the sentence.
\begin{center}
\fbox{sleepy even late fail}
\end{center}
The last step is to apply stemming to all the able remaining words, in this case, the adjective ``sleepy'' stems from sleep.
\begin{center}
\fbox{sleepi even late fail}
\end{center}
Lemmatization\footnote{Natural language tool which returns words as they appear on the dictionary. Uses an internal thesaurus \citep{rf23}.} could also work instead of stemming but the latter was chosen instead because of the problems with the poor ortographical quality of the datasets used in this project, the fact that this method uses a thesaurus instead of arbitrarily trimming the affixes of words would lead to more inaccuracies.

\subsection{Neural Network}
For this project, as mentioned in \hyperref[ch3]{Chapter 3}, TensorFlow was opted for because of its characteristics such as being open-source, not needing a lot of resources to work and the advantages of being portable once trained. All of these traits are what makes this project unique and easily scalable.
An LSTM Neural Network was opted for because of the increased accuracy that it offers compared to a regular Recurrent Neural Network.
\pagebreak
\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.6]{Normal_RNN}
	\caption[Basic Structure of a Recurrent Neural Network.]{Basic Structure of a Recurrent Neural Network, where \textit{A} represents the algorithm used.}
	\label{fig:neuraldiagram_1}
\end{figure}
\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.6]{Transp_RNN}
	\caption[Structure inside an algorithm in a basic Recurrent Neural Network.]{Structure inside an algorithm in a basic Recurrent Neural Network, where \textit{L} represents the layers used.}
	\label{fig:neuraldiagram_2}
\end{figure}
\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.6]{Transp_LSTM}
	\caption[Structure inside an LSTM Neural Network.]{Structure inside an LSTM Neural Network, where the \textit{o} represent the functions that operate the data the data when traveling from one layer to another \citep{rf21}.}
	\label{fig:neuraldiagram_3}
\end{figure}

Basically, LSTM is a subtype of a Recurrent Neural Network which has a certain amount of data be stored for longer periods of time so it can be used for future connections.

The project proposed in this thesis uses a bidirectional LSTM, this is better suited for ``contextualizing'', which is taking into consideration the whole content of the analyzed data as opposed to the unidirectional LSTM which only looks at past data.
Both work the same way, the only change that happens is that weights can be influenced by all data in general at once as opposed to one-at-the time.

This project also makes use of the dropout function, which makes established connections between neurons ``thin out'' at random, with the purpose of avoiding overfitting over time which could ultimately cause problems when using big datasets \citep{rf22}.

\section{Tools}
This project is built on Python v3.8.10, The libraries used for this project to come to fruition are TensorFlow\footnote{\url{https://www.tensorflow.org/}} v2.6.0 and Keras\footnote{\url{https://keras.io/}} v2.6.0 for the Neural Network section. Natural Language Toolkit\footnote{\url{https://www.nltk.org/}} v3.5 (also known as NLTK) for the tokenization and stemming process. Chatterbot\footnote{\url{https://chatterbot.readthedocs.io/en/stable/}} for the optional chatbot module's output. And, lastly, pygame\footnote{\url{https://www.pygame.org/news}} v1.9.5 for the optional GUI.

\section{Inner Workings}
In this section, I will highlight the most important parts of this project's code and their function. In case of needing further insight on the code used, the repository is online at \url{https://github.com/Alex-Ego/Affective-Computing-VN}.
\subsection{Text Filtering}
After the dataset has been properly located and ready to be used, the cleanup discussed earlier in this chapter happens in the following code snippet:
\begin{lstlisting}
def tokenizing_process(message):
    # Pre-tokenizing
    tokens = word_tokenize(message)
    
    # Making them lowercase
    tokens = [w.lower() for w in tokens]
    
    # Filtering the punctuations
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in tokens]
    
    # Filtering non-alphabetic characters
    words = [word for word in stripped if word.isalpha()]
    
    # Removing stopwords
    stop_words = set(stopwords.words('english'))
    words = [w for w in words if not w in stop_words]
    
    # Stemming words
    porter = PorterStemmer()
    stemmed = [porter.stem(word) for word in words]
    
    # Joining the resulting string
    message = " ".join(stemmed)
    return message
    
\end{lstlisting}
This works in the same way and order as specified earlier in \ref{txtfilter} \nameref{txtfilter}.
\subsection{Neural Network}
After the text has been properly classified and ready-to-test, this is the structure of the neural network.
\begin{lstlisting}
model = tf.keras.Sequential([
    layers.Embedding(input_dim=vocab_size, 
                           output_dim=embedding_dim, 
                           input_length=max_length),
    layers.SpatialDropout1D(0.15),
    layers.Bidirectional(layers.LSTM(32, dropout=0.15,
recurrent_dropout=0.15)),
    layers.Dense(8, activation="tanh"),
    layers.Dense(4, activation="softmax")
])
\end{lstlisting}
As the code shown indicates, this neural network has 5 layers: Embedding which processes and either pads or trims the input to be ready to be worked on by the rest of the model, Dropout for avoiding overfitting from previous epochs, Bidirectional LSTM for classification, and two Dense: the former is to slim down the input data from the previous layer, and the latter is for classification which can be one in 4 categories, which include ``Good'', ``Neutral'', ``Bad'', and one reserved for error/unknown purposes.

The dropout layer, specifically using \textit{SpatialDropout1D}, prevents the dropout from being too aggresive and decreasing the learning rate. This works by dropping adjacent connections at the time instead of completely random individual elements.
\subsection{Portability}
Training the Neural Network every time is not needed, since there is a way to save the model in a \textit{.hdf5} file and the corpus in a plain \textit{.txt} file with the following code snippet:
\begin{lstlisting}
model_location = os.path.join(abs_location, "nndata/model")
keras.models.save_model(model, model_location + 
"/sentimental_analysis.hdf5")
with open(model_location + "/tokens.txt", "w") as f:
    f.write(tokenizer.to_json())
    f.close()
\end{lstlisting}
This is fairly useful, because training it every time is very time and resource consuming. Having this as an option opens the path for more applications in less powerful systems.