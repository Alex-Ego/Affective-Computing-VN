Default model:
    layers.Embedding(input_dim=vocab_size, 
                           output_dim=embedding_dim, 
                           input_length=max_length),
    layers.SpatialDropout1D(0.15),
    layers.Bidirectional(layers.LSTM(32, dropout=0.15, recurrent_dropout=0.15)),
    layers.Dense(8, activation="tanh"),
    layers.Dense(4, activation="softmax")

=======================================
=======================================
============= EXPERIMENTS =============
=======================================
=======================================


Experiment 1 = First two datasets, all 5 layers, 10 epochs, not padded, ["sadness", "neutral", "happiness", "fun", "worry", "boredom", "joy", "love", "fear"]
	Loss = 0.6857		Val_Loss = 0.8499
	Accuracy = 0.7151	Val_Acc = 0.6308
Veredict: Work ok for kinda everything but sadness. Has potential

Experiment 2 = All datasets, same parameters as last experiment, ["sadness", "neutral", "happiness", "fun", "worry", "boredom"] (no joy, love, or fear)
	Loss = 0.5956		Val_loss = 0.7649
	Accuracy = 0.7576	Val_Acc = 0.6821
Veredict: Defaults to happiness, sometimes it goes over to the 0 category, doesn't predict well

Experiment 3 = Everything is the same except the LSTM layer is now working with 64 units
	Loss = 0.5829		Val_Loss = 0.7373
	Accuracy = 0.7564	Val_Acc = 0.6780
Veredict: Close to no changes from the previous one

Experiment 4 = Same as last one but ["sadness", "neutral", "happiness", "fun", "worry", "boredom", "joy", "love", "fear"]
	Loss = 0.5455		Val_Loss = 0.6704
	Accuracy = 0.7741	Val_ Acc = 0.7210
Veredict: Same as Experiment 1, so it seems like the full spectrum of emotions is actually helping, but sadness is still underrepresented

Experiment 5 = Same as last one but added a new dataset with ONLY sadness prompts to stave off the lopsided data
	Loss = 0.5442		Val_Loss = 0.6620
	Accuracy = 0.6620	Val_Acc = 0.7162
Veredict: No change in recognition patterns actually. But overall the one with best results.

Experiment 6 = Same as last two, but eliminated the "neutral" category from everything.
	Loss = 0.6222		Val_Loss = 0.7186
	Accuracy = 0.6550	Val_Acc = 0.5357
Veredict: Actually the WORST so far, doesn't recognize anything correctly.

Experiment 7 = Same as 5 but with 5 epochs
	*data already in LaTeX*
Veredict: Largely the same as 5, this means there's close to no overfitting happening.
